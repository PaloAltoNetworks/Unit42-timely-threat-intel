# 2025-12-15 (Monday): Real-World Example of Malicious Indirect Prompt Injection

## Authors:

- Beliz Kaleli, Shehroze Farooqi, Oleksii, Alex Starov

## Notes:

- We identified a real-world example of malicious indirect prompt injection.
- In this example, the actors attempted to bypass AI-based ad reviewers and promote scam products.
- This case uses multiple evasion techniques to hide its injected LLM prompts from security checks:
  - encoding
  - dynamic execution
  - obfuscation
  - semantic tricks
  - visually concealing

## URL For This Example:

- `hxxps[:]//reviewerpress[.]com/advertorial-maxvision-can/?lang=en`

![image](<img alt="2025-12-15-real-world-case-of-malicious-indirect-prompt-injection-image-01" src="https://github.com/user-attachments/assets/3caf8681-b665-461c-98bf-3fe3360fe572" />)
